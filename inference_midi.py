"""
 Gary Plunkett
 Code for wavenet inference. 
 Test files should have been generated by preprocess_maestro script
"""

import os
import json
import csv
import math
from scipy.io.wavfile import write
import numpy as np
import torch
import torch.nn.functional as F
import utils
import debug
from nn.discretized_mix_logistics import SampleDiscretizedMixLogistics

import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from preprocess_groove import Midi2Tensor

def inference(file, test_dir, model_filename, device, output_dir,
              use_conditioning,  use_logistic_mix,
              teacher_force=False, teacher_length=-1, use_train_mode=False,
              no_pedal=False, audio_hz=16000, generation_length=4):

    """
    Run inference on midi data. Test data should be generated by preprocess_maestro in test mode.

    - Setting use_train_mode=True will output teacher forced train-time audio (fast model quality check)

    - Setting teacher_force=True will use teacher forcing in inference mode. This is useful in conjunction
          with setting teacher_length less than teacher_audio's length. Using teacher forcing, then switching
          to autoregressive mode shortly after, gives autoregression a good history of samples to start with.

    - Use no_pedal to remove pedal data. Midi data saved with pedal information, but if excluded from training
          make surre to set it here too.
    """
    
    model = torch.load(model_filename)['model'].to(device)
    
    # canvas to plot midi roll on
    fig, ax = plt.subplots()

    # Open the metadata csv and get file informationx
    midiX = None
    if use_conditioning:
        midiX = Midi2Tensor(file,16000,False)
        midiX = torch.from_numpy(midiX.todense())
        
        midiX = midiX[:88]
    
        # plot midi features
        plt.cla()
        ax.spy(midiX, markersize=3, aspect="auto", origin='lower')
        plt.savefig(file + "_midiroll.png")

        midiX = midiX.unsqueeze(0).to(device)
    else:
        midiX = None
    
    if use_conditioning:
        audio = model.inference(midiX)
    else:
        audio = model.inference(midiX, length=4, device=device, batch_size=1)

    audio = audio.squeeze().cpu().numpy()
    
    write(file + "_inference.wav", audio_hz, audio)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', type=str, help="Location of configuration file")   
    parser.add_argument('-f', '--file', type=str, help="Midi File")  
    args = parser.parse_args()

    with (open(args.config)) as f:
        data = f.read()
    config = json.loads(data)["inference_config"]
    
    inference(args.file,**config)
